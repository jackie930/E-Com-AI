{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddlePaddle BYOS\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "This notebook shows how to use the SageMaker Python SDK to run your code in a local container before deploying to SageMaker's managed training or hosting environments.  This can speed up iterative testing and debugging while using the same familiar Python SDK interface.  Just change your estimator's `train_instance_type` to `local` (or `local_gpu` if you're using an ml.p2 or ml.p3 notebook instance).\n",
    "\n",
    "In order to use this feature you'll need to install docker-compose (and nvidia-docker if training with a GPU).\n",
    "\n",
    "**Note, you can only run a single local notebook at one time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/bin/bash ./utils/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install paddlepaddle paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **SageMaker Python SDK** helps you deploy your models for training and hosting in optimized, productions ready containers in SageMaker. The SageMaker Python SDK is easy to use, modular, extensible and compatible with TensorFlow, MXNet, PyTorch and Chainer. This tutorial focuses on how to create a convolutional neural network model to train the [Cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) using **PyTorch in local mode**.\n",
    "\n",
    "### Set up the environment\n",
    "\n",
    "This notebook was created and tested on a single ml.p2.xlarge notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/shulex-jackie'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鞋all.jsonl  鞋-标注.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls ./shulex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepare_shulex.py \\\n",
    "    --input_path './shulex/鞋-标注.jsonl' \\\n",
    "    --output_folder './output_shulex'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-10-13 09:25:59,473] [    INFO]\u001b[0m - Converting doccano data...\u001b[0m\n",
      "100%|████████████████████████████████████████| 18/18 [00:00<00:00, 31339.76it/s]\n",
      "\u001b[32m[2022-10-13 09:25:59,475] [    INFO]\u001b[0m - Adding negative samples for first stage prompt...\u001b[0m\n",
      "100%|███████████████████████████████████████| 18/18 [00:00<00:00, 128179.07it/s]\n",
      "\u001b[32m[2022-10-13 09:25:59,476] [    INFO]\u001b[0m - Converting doccano data...\u001b[0m\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 16946.68it/s]\n",
      "\u001b[32m[2022-10-13 09:25:59,476] [    INFO]\u001b[0m - Adding negative samples for first stage prompt...\u001b[0m\n",
      "100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 44150.57it/s]\n",
      "\u001b[32m[2022-10-13 09:25:59,477] [    INFO]\u001b[0m - Converting doccano data...\u001b[0m\n",
      "0it [00:00, ?it/s]\n",
      "\u001b[32m[2022-10-13 09:25:59,477] [    INFO]\u001b[0m - Adding negative samples for first stage prompt...\u001b[0m\n",
      "0it [00:00, ?it/s]\n",
      "\u001b[32m[2022-10-13 09:25:59,479] [    INFO]\u001b[0m - Save 90 examples to ./data_shulex/train.txt.\u001b[0m\n",
      "\u001b[32m[2022-10-13 09:25:59,479] [    INFO]\u001b[0m - Save 10 examples to ./data_shulex/dev.txt.\u001b[0m\n",
      "\u001b[32m[2022-10-13 09:25:59,479] [    INFO]\u001b[0m - Save 0 examples to ./data_shulex/test.txt.\u001b[0m\n",
      "\u001b[32m[2022-10-13 09:25:59,479] [    INFO]\u001b[0m - Finished! It takes 0.01 seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python doccano.py \\\n",
    "    --folder_path ./output_shulex \\\n",
    "    --task_type ext \\\n",
    "    --save_dir ./data_shulex \\\n",
    "    --splits 0.9 0.1 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = sagemaker.Session().upload_data(path = \"./data_shulex\", key_prefix=prefix)\n",
    "# base_dir = 'file:///home/ec2-user/SageMaker/paddlenlp_sagemaker/data/'\n",
    "# inputs = {'training': base_dir}\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-064542430558/sagemaker/shulex-jackie'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Functions\n",
    "\n",
    "SageMaker invokes the main function defined within your training script for training. When deploying your trained model to an endpoint, the model_fn() is called to determine how to load your trained model. The model_fn() along with a few other functions list below are called to enable predictions on SageMaker.\n",
    "\n",
    "### [Predicting Functions](https://github.com/aws/sagemaker-pytorch-containers/blob/master/src/sagemaker_pytorch_container/serving.py)\n",
    "* model_fn(model_dir) - loads your model.\n",
    "* input_fn(serialized_input_data, content_type) - deserializes predictions to predict_fn.\n",
    "* output_fn(prediction_output, accept) - serializes predictions from predict_fn.\n",
    "* predict_fn(input_data, model) - calls a model on data deserialized in input_fn.\n",
    "\n",
    "The model_fn() is the only function that doesn't have a default implementation and is required by the user for using PyTorch on SageMaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.PyTorch estimator\n",
    "\n",
    "The `PyTorch` class allows us to run our training function on SageMaker. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. For local training with GPU, we could set this to \"local_gpu\".  In this case, `instance_type` was set above based on your whether you're running a GPU instance.\n",
    "\n",
    "After we've constructed our `PyTorch` object, we fit it using the data we uploaded to S3. Even though we're in local mode, using S3 as our data source makes sense because it maintains consistency with how SageMaker's distributed, managed training ingests data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training using GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 's3://sagemaker-us-west-2-064542430558/sagemaker/shulex-jackie'}\n"
     ]
    }
   ],
   "source": [
    "inputs = {'training': data_location}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload uie-base-en pretrain\n",
    "\n",
    "# uie_en_model_s3 = sagemaker.Session().upload_data(path = \"../uie-base-en/taskflow/information_extraction/uie-base-en\", key_prefix=\"model_uie_base_en\")\n",
    "uie_en_model_s3 = 's3://sagemaker-us-west-2-064542430558/model_uie_base_en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-13 09:27:36 Starting - Starting the training job...\n",
      "2022-10-13 09:28:02 Starting - Preparing the instances for trainingProfilerReport-1665653255: InProgress\n",
      ".........\n",
      "2022-10-13 09:29:23 Downloading - Downloading input data...\n",
      "2022-10-13 09:30:03 Training - Downloading the training image..................\n",
      "2022-10-13 09:33:03 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-10-13 09:32:53,743 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-10-13 09:32:53,771 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-10-13 09:32:53,777 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-10-13 09:33:08,680 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://opentuna.cn/pypi/web/simple/\u001b[0m\n",
      "\u001b[34mCollecting paddlepaddle-gpu\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/fb/f2/6b6ae62d5ecd916d61e1f527cb14990038d473cc670c30045f80b557e6e1/paddlepaddle_gpu-2.3.1-cp38-cp38-manylinux1_x86_64.whl (393.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 393.9/393.9 MB 1.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting paddlenlp\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/8e/e1/94cdbaca400a57687a8529213776468f003b64b6e35a6f4acf6b6539f543/paddlenlp-2.3.4-py3-none-any.whl (1.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 1.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting protobuf<=3.20.0,>=3.1.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/88/88/cd55f87e896b82a3aba8e6c0affc077de51f7321cf730622b17ef7b0f69c/protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 3.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.13 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (1.22.3)\u001b[0m\n",
      "\u001b[34mCollecting paddle-bfloat==0.1.7\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/76/d7/ba0e1aeec33e20c78af5cf2fdbb7e7cabfe4679557e68759a17c97e03540/paddle_bfloat-0.1.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 385.5/385.5 kB 8.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (9.1.1)\u001b[0m\n",
      "\u001b[34mCollecting astor\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting opt-einsum==3.3.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 6.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (5.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (2.27.1)\u001b[0m\n",
      "\u001b[34mCollecting paddlefsl\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/fb/4a/25d1959a8f1fe5ee400f32fc9fc8b56d4fd6fc25315e23c0171f6e705e2a/paddlefsl-1.1.0-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.0/101.0 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting paddle2onnx\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/78/76/811c8c897d68e211bc7ba13fa6161f54747eb717bffae80db0ea09ca2e43/paddle2onnx-0.9.8-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 9.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multiprocess<=0.70.12.2\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/e6/22/b09b8394f8c86ff0cfebd725ea96bba0accd4a4b2be437bcba6a0cf7d1c3/multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.3/128.3 kB 13.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/68/91/ded0f64f90abfc5413c620fc345a0aef1e7ff5addda8704cc6b3bf589c64/sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 15.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting colorlog\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/7d/54/e24efe5469ecb2710112055de87a2900e9494810bcfc25c12c7a0723eb64/colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting dill<0.3.5\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/b6/c3/973676ceb86b60835bb3978c6db67a5dc06be6cfdbd14ef0f5a13e3fc9fd/dill-0.3.4-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.9/86.9 kB 11.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (0.4.4)\u001b[0m\n",
      "\u001b[34mCollecting seqeval\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 8.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/98/29/f381f8a633fed2c4f41c191498c3bc43d91a8e44c5202a8b0b2bd8b1acf3/datasets-2.3.2-py3-none-any.whl (362 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 642.7 kB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (4.61.2)\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 16.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/6a/cf/50f4cfde85d90c2b3e9c98b46e17d190bbdd97b54d3e0876e1d9360e487f/xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 5.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (2022.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (6.0.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/d8/2c/9af8451ab780598e3b26a84d4f0e3844841456657401eb6843fdb622bb41/huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 kB 4.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (1.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (21.2)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/38/71/e1db3f96fa85f77906ef002a08fa8d02dbdb3292180d41eb1b17ddab72bf/aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 3.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tqdm\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/8a/c4/d15f1e627fff25443ded77ea70a7b5532d6371498f9285d44d62587e209c/tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 10.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (2022.5.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.8/site-packages (from seqeval->paddlenlp->-r requirements.txt (line 3)) (1.0.2)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/a6/d5/17f02b379525d1ff9678bfa58eb9548f561c8826deb0b85797aa0eed582d/filelock-3.7.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 3)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 3)) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/3b/76/3d7c273b91e6dc914859f8752d42b763f39ae83782ec9a063a526c816977/frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/aa/a6/a4ddcb1c3d93fc5d77a19b1ec338a3efec65b44345168d8ac9bf8461224a/yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 11.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/3b/87/fe94898f2d44a93a35d5aa74671ed28094d80753a1113d68b799fab6dc22/aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/8f/39/a7e04961b4c00d68aba337e3fdef9fd4f666dcd98f41725067a1de5d3399/multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 7.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (2021.3)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: jieba, seqeval\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=752b3ec94d23563b9b15c812aae2f3676ece7a443c13d37ba44828b291e5b502\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e9/6d/0d/92e938a9f51144388ebba6a81ebe4206fdd93f9c9de1434ec2\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=8df61b6c66fe7465f46a18daddbecac6e3ec96eaef966b95eb385861dc49fa1b\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/6d/ff/58/c7ebcaa099f483531e06ca79ad5802e594d1c97c96c9c0f200\u001b[0m\n",
      "\u001b[34mSuccessfully built jieba seqeval\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, paddle2onnx, paddle-bfloat, jieba, xxhash, tqdm, protobuf, opt-einsum, multidict, frozenlist, filelock, dill, colorlog, async-timeout, astor, yarl, responses, paddlepaddle-gpu, paddlefsl, multiprocess, huggingface-hub, aiosignal, seqeval, aiohttp, datasets, paddlenlp\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.61.2\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.61.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: protobuf\u001b[0m\n",
      "\u001b[34mFound existing installation: protobuf 3.20.1\u001b[0m\n",
      "\u001b[34mUninstalling protobuf-3.20.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled protobuf-3.20.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: dill\u001b[0m\n",
      "\u001b[34mFound existing installation: dill 0.3.5.1\u001b[0m\n",
      "\u001b[34mUninstalling dill-0.3.5.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled dill-0.3.5.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: multiprocess\u001b[0m\n",
      "\u001b[34mFound existing installation: multiprocess 0.70.13\u001b[0m\n",
      "\u001b[34mUninstalling multiprocess-0.70.13:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled multiprocess-0.70.13\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mpathos 0.2.9 requires dill>=0.3.5.1, but you have dill 0.3.4 which is incompatible.\u001b[0m\n",
      "\u001b[34mpathos 0.2.9 requires multiprocess>=0.70.13, but you have multiprocess 0.70.12.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.1 aiosignal-1.2.0 astor-0.8.1 async-timeout-4.0.2 colorlog-6.6.0 datasets-2.3.2 dill-0.3.4 filelock-3.7.1 frozenlist-1.3.0 huggingface-hub-0.8.1 jieba-0.42.1 multidict-6.0.2 multiprocess-0.70.12.2 opt-einsum-3.3.0 paddle-bfloat-0.1.7 paddle2onnx-0.9.8 paddlefsl-1.1.0 paddlenlp-2.3.4 paddlepaddle-gpu-2.3.1 protobuf-3.20.0 responses-0.18.0 sentencepiece-0.1.96 seqeval-1.2.2 tqdm-4.64.0 xxhash-3.0.0 yarl-1.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.1.2 -> 22.2.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-10-13 09:34:34,855 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-10-13 09:34:34,855 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-10-13 09:34:34,938 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 16,\n",
      "        \"dev_path\": \"/opt/ml/input/data/training/dev.txt\",\n",
      "        \"device\": \"gpu\",\n",
      "        \"freeze\": true,\n",
      "        \"learning_rate\": 1e-05,\n",
      "        \"logging_steps\": 10,\n",
      "        \"max_seq_len\": 512,\n",
      "        \"model\": \"uie-base\",\n",
      "        \"num_epochs\": 50,\n",
      "        \"save_dir\": \"/opt/ml/model\",\n",
      "        \"seed\": 1000,\n",
      "        \"train_path\": \"/opt/ml/input/data/training/train.txt\",\n",
      "        \"valid_steps\": 50\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-10-13-09-26-06-200\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-064542430558/pytorch-training-2022-10-13-09-26-06-200/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"finetune\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"finetune.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":16,\"dev_path\":\"/opt/ml/input/data/training/dev.txt\",\"device\":\"gpu\",\"freeze\":true,\"learning_rate\":1e-05,\"logging_steps\":10,\"max_seq_len\":512,\"model\":\"uie-base\",\"num_epochs\":50,\"save_dir\":\"/opt/ml/model\",\"seed\":1000,\"train_path\":\"/opt/ml/input/data/training/train.txt\",\"valid_steps\":50}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=finetune.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=finetune\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-064542430558/pytorch-training-2022-10-13-09-26-06-200/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":16,\"dev_path\":\"/opt/ml/input/data/training/dev.txt\",\"device\":\"gpu\",\"freeze\":true,\"learning_rate\":1e-05,\"logging_steps\":10,\"max_seq_len\":512,\"model\":\"uie-base\",\"num_epochs\":50,\"save_dir\":\"/opt/ml/model\",\"seed\":1000,\"train_path\":\"/opt/ml/input/data/training/train.txt\",\"valid_steps\":50},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-10-13-09-26-06-200\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-064542430558/pytorch-training-2022-10-13-09-26-06-200/source/sourcedir.tar.gz\",\"module_name\":\"finetune\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"finetune.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"16\",\"--dev_path\",\"/opt/ml/input/data/training/dev.txt\",\"--device\",\"gpu\",\"--freeze\",\"True\",\"--learning_rate\",\"1e-05\",\"--logging_steps\",\"10\",\"--max_seq_len\",\"512\",\"--model\",\"uie-base\",\"--num_epochs\",\"50\",\"--save_dir\",\"/opt/ml/model\",\"--seed\",\"1000\",\"--train_path\",\"/opt/ml/input/data/training/train.txt\",\"--valid_steps\",\"50\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_DEV_PATH=/opt/ml/input/data/training/dev.txt\u001b[0m\n",
      "\u001b[34mSM_HP_DEVICE=gpu\u001b[0m\n",
      "\u001b[34mSM_HP_FREEZE=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=10\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LEN=512\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL=uie-base\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=50\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=1000\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_PATH=/opt/ml/input/data/training/train.txt\u001b[0m\n",
      "\u001b[34mSM_HP_VALID_STEPS=50\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python finetune.py --batch_size 16 --dev_path /opt/ml/input/data/training/dev.txt --device gpu --freeze True --learning_rate 1e-05 --logging_steps 10 --max_seq_len 512 --model uie-base --num_epochs 50 --save_dir /opt/ml/model --seed 1000 --train_path /opt/ml/input/data/training/train.txt --valid_steps 50\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:34:38,629] [    INFO]#033[0m - Downloading resource files...#033[0m\u001b[0m\n",
      "\u001b[34m0%|          | 0/460749 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 3/460749 [00:00<9:38:11, 13.28it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 35/460749 [00:00<1:27:07, 88.14it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 67/460749 [00:00<1:08:37, 111.88it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 99/460749 [00:00<57:08, 134.38it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 147/460749 [00:01<50:57, 150.65it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 179/460749 [00:01<45:42, 167.91it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 227/460749 [00:01<44:15, 173.39it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 275/460749 [00:01<44:22, 172.93it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 323/460749 [00:02<41:41, 184.06it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 371/460749 [00:02<37:21, 205.34it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 435/460749 [00:02<35:50, 214.03it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 483/460749 [00:02<36:02, 212.85it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 547/460749 [00:02<32:58, 232.62it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 611/460749 [00:03<31:04, 246.73it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 675/460749 [00:03<29:52, 256.64it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 739/460749 [00:03<29:05, 263.58it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 803/460749 [00:03<28:31, 268.68it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 883/460749 [00:04<26:10, 292.79it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 963/460749 [00:04<24:44, 309.82it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1043/460749 [00:04<23:48, 321.83it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1123/460749 [00:04<23:10, 330.48it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1219/460749 [00:05<21:28, 356.56it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1315/460749 [00:05<20:24, 375.34it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1411/460749 [00:05<18:14, 419.75it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1523/460749 [00:05<15:11, 503.85it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1578/460749 [00:05<16:14, 471.00it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1651/460749 [00:05<16:40, 459.01it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1795/460749 [00:06<16:00, 477.58it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1939/460749 [00:06<13:22, 571.85it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 2099/460749 [00:06<10:50, 704.67it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 2175/460749 [00:06<11:40, 655.03it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 2291/460749 [00:06<12:39, 603.44it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 2483/460749 [00:07<11:09, 684.54it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 2691/460749 [00:07<09:15, 824.66it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 2899/460749 [00:07<07:14, 1053.96it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3021/460749 [00:07<08:16, 922.02it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3171/460749 [00:07<08:23, 908.35it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3427/460749 [00:07<06:37, 1150.78it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3553/460749 [00:08<07:08, 1066.36it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3731/460749 [00:08<07:09, 1063.19it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4003/460749 [00:08<05:24, 1406.97it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4161/460749 [00:08<06:18, 1206.12it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4387/460749 [00:08<06:06, 1246.22it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4691/460749 [00:08<04:40, 1623.08it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4877/460749 [00:08<05:26, 1396.67it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5139/460749 [00:09<04:47, 1582.19it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5315/460749 [00:09<04:45, 1597.54it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5571/460749 [00:09<04:50, 1567.78it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5971/460749 [00:09<03:34, 2119.96it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 6207/460749 [00:09<04:13, 1796.57it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 6515/460749 [00:09<04:12, 1797.36it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6995/460749 [00:09<03:05, 2446.44it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7276/460749 [00:10<03:35, 2099.78it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7603/460749 [00:10<03:41, 2041.52it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8147/460749 [00:10<02:43, 2764.73it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8469/460749 [00:10<03:09, 2384.26it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8835/460749 [00:10<03:15, 2307.51it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9491/460749 [00:10<02:31, 2977.94it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9816/460749 [00:10<02:44, 2736.03it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10243/460749 [00:11<02:49, 2657.12it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10979/460749 [00:11<02:12, 3399.97it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 11337/460749 [00:11<02:25, 3094.07it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11827/460749 [00:11<02:28, 3021.76it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12633/460749 [00:11<01:49, 4102.59it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13093/460749 [00:11<02:07, 3502.84it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13603/460749 [00:12<02:13, 3344.77it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 14499/460749 [00:12<01:38, 4533.51it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 15024/460749 [00:12<01:54, 3905.87it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 15587/460749 [00:12<01:48, 4087.81it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 16042/460749 [00:12<01:47, 4127.01it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 16675/460749 [00:12<01:40, 4428.80it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 17144/460749 [00:12<01:40, 4409.03it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 17827/460749 [00:12<01:42, 4328.78it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 18883/460749 [00:13<01:16, 5788.02it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 19504/460749 [00:13<01:32, 4782.56it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 20307/460749 [00:13<01:31, 4825.29it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 21491/460749 [00:13<01:09, 6342.72it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 22202/460749 [00:13<01:21, 5349.99it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 23043/460749 [00:13<01:15, 5760.97it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 23679/460749 [00:13<01:15, 5782.32it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 24531/460749 [00:14<01:10, 6150.40it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 25179/460749 [00:14<01:11, 6105.33it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 26099/460749 [00:14<01:06, 6552.71it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 26771/460749 [00:14<01:07, 6450.43it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 27731/460749 [00:14<01:02, 6920.23it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 28430/460749 [00:14<01:03, 6783.32it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 29459/460749 [00:14<00:58, 7331.67it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 30194/460749 [00:14<01:00, 7168.29it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 31267/460749 [00:14<00:55, 7720.81it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 32038/460749 [00:15<00:57, 7419.37it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 33171/460749 [00:15<00:52, 8158.39it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 33987/460749 [00:15<00:53, 7964.70it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 35155/460749 [00:15<00:50, 8497.37it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 36002/460749 [00:15<00:51, 8311.25it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 37251/460749 [00:15<00:47, 8950.15it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 38142/460749 [00:15<00:48, 8724.78it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 39443/460749 [00:15<00:44, 9395.14it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 40378/460749 [00:16<00:45, 9146.70it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 41747/460749 [00:16<00:42, 9852.73it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 42727/460749 [00:16<00:43, 9575.41it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 44163/460749 [00:16<00:40, 10360.68it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 45194/460749 [00:16<00:41, 10064.27it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 46707/460749 [00:16<00:42, 9740.38it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 48995/460749 [00:16<00:31, 13008.55it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 50364/460749 [00:16<00:38, 10614.32it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 52115/460749 [00:17<00:34, 11701.59it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 53378/460749 [00:17<00:35, 11492.86it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 54995/460749 [00:17<00:36, 11142.68it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 56707/460749 [00:17<00:32, 12538.74it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 58067/460749 [00:17<00:34, 11802.55it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 59811/460749 [00:17<00:30, 13213.26it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 61196/460749 [00:17<00:32, 12343.30it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 62853/460749 [00:17<00:29, 13432.42it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 64249/460749 [00:18<00:31, 12599.48it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 65552/460749 [00:18<00:31, 12691.25it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 66853/460749 [00:18<00:33, 11854.75it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 68553/460749 [00:18<00:29, 13220.51it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 69913/460749 [00:18<00:31, 12242.78it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 71267/460749 [00:18<00:33, 11658.63it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 72963/460749 [00:18<00:32, 12108.17it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 74291/460749 [00:18<00:31, 12407.44it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 75859/460749 [00:18<00:31, 12342.41it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 77283/460749 [00:19<00:29, 12840.08it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 78584/460749 [00:19<00:29, 12886.34it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 80131/460749 [00:19<00:30, 12599.35it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 81403/460749 [00:19<00:30, 12624.23it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 82755/460749 [00:19<00:31, 11925.14it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 84121/460749 [00:19<00:30, 12392.33it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 85375/460749 [00:19<00:32, 11461.52it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 87187/460749 [00:19<00:30, 12358.42it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 88432/460749 [00:19<00:30, 12289.60it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 90179/460749 [00:20<00:28, 12796.55it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 91460/460749 [00:20<00:29, 12651.82it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 92993/460749 [00:20<00:27, 13383.01it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 94338/460749 [00:20<00:29, 12324.64it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 95699/460749 [00:20<00:30, 11972.86it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 96910/460749 [00:20<00:30, 11906.23it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 98659/460749 [00:20<00:28, 12562.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 99917/460749 [00:20<00:32, 11221.44it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 101939/460749 [00:21<00:27, 13064.77it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 103273/460749 [00:21<00:31, 11309.49it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 105447/460749 [00:21<00:25, 13838.76it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 106926/460749 [00:21<00:27, 12733.62it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 108403/460749 [00:21<00:28, 12384.10it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 109695/460749 [00:21<00:28, 12502.51it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 111091/460749 [00:21<00:29, 11966.21it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 112579/460749 [00:21<00:29, 11950.27it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 114067/460749 [00:22<00:27, 12586.28it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 115539/460749 [00:22<00:27, 12333.68it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 116947/460749 [00:22<00:27, 12672.27it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 118563/460749 [00:22<00:26, 12753.91it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 119850/460749 [00:22<00:27, 12608.80it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 121427/460749 [00:22<00:26, 12659.21it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 122697/460749 [00:22<00:27, 12456.57it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 124307/460749 [00:22<00:26, 12678.76it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 125575/460749 [00:22<00:26, 12414.33it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 127091/460749 [00:23<00:26, 12459.46it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 128336/460749 [00:23<00:27, 12066.02it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 129683/460749 [00:23<00:27, 11848.63it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 131123/460749 [00:23<00:26, 12474.00it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 132627/460749 [00:23<00:26, 12276.53it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 134035/460749 [00:23<00:25, 12685.70it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 135571/460749 [00:23<00:25, 12510.52it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 136979/460749 [00:23<00:25, 12855.15it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 138563/460749 [00:24<00:25, 12736.82it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 139844/460749 [00:24<00:25, 12655.18it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 141165/460749 [00:24<00:24, 12807.70it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 142450/460749 [00:24<00:26, 11818.06it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 144131/460749 [00:24<00:25, 12370.79it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 145376/460749 [00:24<00:25, 12207.27it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 147059/460749 [00:24<00:24, 12673.05it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 148327/460749 [00:24<00:24, 12526.51it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 149891/460749 [00:24<00:24, 12475.83it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 151187/460749 [00:25<00:24, 12584.34it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 152643/460749 [00:25<00:25, 12154.08it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 154131/460749 [00:25<00:24, 12610.77it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 155539/460749 [00:25<00:24, 12266.42it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 156995/460749 [00:25<00:24, 12605.00it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 158387/460749 [00:25<00:24, 12234.78it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 159779/460749 [00:25<00:24, 12412.63it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 161235/460749 [00:25<00:24, 12279.34it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 162611/460749 [00:25<00:24, 12393.65it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 164099/460749 [00:26<00:24, 12326.92it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 165427/460749 [00:26<00:24, 12301.48it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 166851/460749 [00:26<00:24, 12131.58it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 168307/460749 [00:26<00:23, 12494.15it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 169619/460749 [00:26<00:24, 12010.36it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 171171/460749 [00:26<00:22, 12623.21it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 172451/460749 [00:26<00:23, 12049.03it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 173987/460749 [00:26<00:22, 12566.96it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 175249/460749 [00:26<00:23, 12000.63it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 176835/460749 [00:27<00:22, 12637.48it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 178104/460749 [00:27<00:23, 12090.80it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 179619/460749 [00:27<00:22, 12486.94it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 180963/460749 [00:27<00:22, 12243.28it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 182467/460749 [00:27<00:22, 12498.74it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 183827/460749 [00:27<00:22, 12338.81it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 185475/460749 [00:27<00:21, 12937.94it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 186770/460749 [00:27<00:22, 12438.99it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 188387/460749 [00:28<00:21, 12965.79it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 189685/460749 [00:28<00:21, 12447.73it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 191363/460749 [00:28<00:20, 13154.78it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 192680/460749 [00:28<00:21, 12498.29it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 194179/460749 [00:28<00:20, 12849.29it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 195468/460749 [00:28<00:21, 12125.91it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 196915/460749 [00:28<00:21, 12538.33it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 198176/460749 [00:28<00:22, 11757.69it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 199827/460749 [00:28<00:20, 12887.04it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 201133/460749 [00:29<00:21, 12160.13it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 202643/460749 [00:29<00:20, 12683.56it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 203926/460749 [00:29<00:21, 12210.88it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 205587/460749 [00:29<00:19, 12935.18it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 206888/460749 [00:29<00:20, 12395.63it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 208435/460749 [00:29<00:19, 12803.11it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 209721/460749 [00:29<00:20, 12200.29it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 211491/460749 [00:29<00:18, 13330.77it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 212832/460749 [00:29<00:20, 12200.19it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 214739/460749 [00:30<00:20, 12101.36it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 216437/460749 [00:30<00:18, 13297.31it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 217804/460749 [00:30<00:19, 12415.93it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 219473/460749 [00:30<00:17, 13508.50it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 220864/460749 [00:30<00:19, 12540.41it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 222547/460749 [00:30<00:19, 12259.19it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 223907/460749 [00:30<00:20, 11727.83it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 225411/460749 [00:30<00:18, 12554.01it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 226819/460749 [00:31<00:19, 12046.10it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 228291/460749 [00:31<00:18, 12737.02it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 229699/460749 [00:31<00:19, 12157.85it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 231301/460749 [00:31<00:17, 13175.63it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 232649/460749 [00:31<00:18, 12239.87it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 234354/460749 [00:31<00:16, 13515.00it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 235743/460749 [00:31<00:18, 12431.67it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 237475/460749 [00:31<00:16, 13227.52it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 238828/460749 [00:32<00:18, 12309.83it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 240405/460749 [00:32<00:16, 13210.52it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 241759/460749 [00:32<00:17, 12222.99it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 243265/460749 [00:32<00:16, 12967.49it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 244595/460749 [00:32<00:18, 11879.86it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 246275/460749 [00:32<00:18, 11751.09it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 248299/460749 [00:32<00:15, 13874.78it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 249748/460749 [00:32<00:18, 11636.49it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 251523/460749 [00:33<00:16, 13076.30it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 252934/460749 [00:33<00:16, 12313.48it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 254419/460749 [00:33<00:15, 12948.55it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 255779/460749 [00:33<00:17, 11728.47it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 257333/460749 [00:33<00:16, 12687.01it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 258663/460749 [00:33<00:16, 12515.68it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 260099/460749 [00:33<00:15, 12922.80it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 261523/460749 [00:33<00:15, 12463.96it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 263091/460749 [00:33<00:15, 13161.36it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 264431/460749 [00:34<00:15, 12339.69it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 266115/460749 [00:34<00:14, 13456.52it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 267488/460749 [00:34<00:15, 12338.39it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 269283/460749 [00:34<00:14, 13475.99it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 270660/460749 [00:34<00:15, 12455.58it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 272540/460749 [00:34<00:13, 14112.09it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 273997/460749 [00:34<00:14, 12767.93it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 275539/460749 [00:34<00:14, 12489.36it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 276824/460749 [00:35<00:14, 12491.56it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 278227/460749 [00:35<00:15, 12067.83it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 279454/460749 [00:35<00:14, 12115.77it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 280947/460749 [00:35<00:15, 11927.22it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 282451/460749 [00:35<00:14, 12369.30it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 283987/460749 [00:35<00:14, 12597.92it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 285427/460749 [00:35<00:13, 12687.30it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 287027/460749 [00:35<00:13, 12983.72it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 288435/460749 [00:35<00:13, 12881.20it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 290035/460749 [00:36<00:13, 13122.06it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 291349/460749 [00:36<00:12, 13118.05it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 292963/460749 [00:36<00:12, 12930.51it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 294259/460749 [00:36<00:13, 12800.31it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 295811/460749 [00:36<00:12, 12691.66it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 297082/460749 [00:36<00:13, 12292.26it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 298579/460749 [00:36<00:13, 12463.95it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 299826/460749 [00:36<00:13, 11979.35it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 301395/460749 [00:36<00:12, 12477.43it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 302739/460749 [00:37<00:12, 12338.72it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 304419/460749 [00:37<00:12, 12965.60it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 305795/460749 [00:37<00:12, 12765.62it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 307459/460749 [00:37<00:11, 13222.67it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 308787/460749 [00:37<00:11, 12820.40it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 310483/460749 [00:37<00:11, 13348.30it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 311816/460749 [00:37<00:11, 12823.72it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 313331/460749 [00:37<00:11, 12969.19it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 314628/460749 [00:38<00:11, 12478.72it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 315955/460749 [00:38<00:11, 12215.77it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 317177/460749 [00:38<00:12, 11732.82it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 318659/460749 [00:38<00:11, 12081.39it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 320227/460749 [00:38<00:11, 12597.28it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 321667/460749 [00:38<00:11, 12572.80it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 323251/460749 [00:38<00:10, 12977.83it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 324675/460749 [00:38<00:10, 12800.36it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 326291/460749 [00:38<00:10, 13215.30it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 327613/460749 [00:39<00:10, 12703.12it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 329235/460749 [00:39<00:10, 12794.19it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 330515/460749 [00:39<00:10, 12673.32it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 332051/460749 [00:39<00:10, 12498.63it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 333395/460749 [00:39<00:10, 12677.86it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 334899/460749 [00:39<00:10, 12383.42it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 336323/460749 [00:39<00:09, 12845.82it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 337651/460749 [00:39<00:10, 12014.77it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 339107/460749 [00:39<00:10, 12069.84it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 340579/460749 [00:40<00:09, 12466.58it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 341955/460749 [00:40<00:09, 12122.79it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 343443/460749 [00:40<00:10, 11541.47it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 345219/460749 [00:40<00:09, 12161.93it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 346442/460749 [00:40<00:09, 12104.09it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 348227/460749 [00:40<00:08, 12738.43it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 349501/460749 [00:40<00:08, 12603.85it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 351123/460749 [00:40<00:08, 12729.59it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 352395/460749 [00:41<00:08, 12568.60it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 353827/460749 [00:41<00:08, 12246.08it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 355052/460749 [00:41<00:08, 11752.68it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 356515/460749 [00:41<00:08, 12104.76it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 357923/460749 [00:41<00:08, 12590.31it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 359427/460749 [00:41<00:08, 12339.37it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 360931/460749 [00:41<00:07, 13014.16it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 362244/460749 [00:41<00:08, 12065.35it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 363983/460749 [00:41<00:07, 13492.41it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 365361/460749 [00:42<00:07, 12374.78it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 366659/460749 [00:42<00:07, 12288.70it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 367911/460749 [00:42<00:07, 11788.05it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 369555/460749 [00:42<00:07, 12663.21it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 370836/460749 [00:42<00:07, 12134.46it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 372387/460749 [00:42<00:06, 12659.92it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 373662/460749 [00:42<00:07, 12135.28it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 375123/460749 [00:42<00:06, 12402.08it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 376369/460749 [00:43<00:07, 11834.09it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 377683/460749 [00:43<00:07, 11823.52it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 379187/460749 [00:43<00:06, 12188.36it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 380579/460749 [00:43<00:06, 12216.73it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 382051/460749 [00:43<00:06, 12383.51it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 383507/460749 [00:43<00:06, 12521.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 384947/460749 [00:43<00:06, 12509.56it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 386275/460749 [00:43<00:05, 12593.53it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 387763/460749 [00:43<00:05, 12372.21it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 389155/460749 [00:44<00:05, 12664.00it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 390483/460749 [00:44<00:05, 12000.37it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 392019/460749 [00:44<00:05, 12783.73it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 393310/460749 [00:44<00:05, 11917.12it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 394931/460749 [00:44<00:05, 13026.84it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 396257/460749 [00:44<00:05, 12068.48it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 397663/460749 [00:44<00:05, 12600.02it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 398948/460749 [00:44<00:05, 11848.29it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 400403/460749 [00:44<00:04, 12436.34it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 401827/460749 [00:45<00:04, 12130.88it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 403331/460749 [00:45<00:04, 12750.29it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 404707/460749 [00:45<00:04, 12228.57it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 406275/460749 [00:45<00:04, 12991.36it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 407590/460749 [00:45<00:04, 12217.89it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 409091/460749 [00:45<00:04, 12819.79it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 410390/460749 [00:45<00:04, 11972.42it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 411747/460749 [00:45<00:03, 12323.27it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 413187/460749 [00:45<00:03, 12108.56it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 414675/460749 [00:46<00:03, 12682.62it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 416227/460749 [00:46<00:03, 12642.23it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 417619/460749 [00:46<00:03, 12812.02it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 419219/460749 [00:46<00:03, 12857.45it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 420515/460749 [00:46<00:03, 12712.82it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 422179/460749 [00:46<00:02, 12948.22it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 423475/460749 [00:46<00:02, 12716.66it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 425075/460749 [00:46<00:02, 12852.67it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 426359/460749 [00:47<00:02, 12604.92it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 427875/460749 [00:47<00:02, 12561.60it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 429130/460749 [00:47<00:02, 12354.52it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 430723/460749 [00:47<00:02, 12564.42it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 431977/460749 [00:47<00:02, 12245.24it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 433555/460749 [00:47<00:02, 12416.13it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 434794/460749 [00:47<00:02, 12232.46it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 436483/460749 [00:47<00:01, 12665.91it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 437745/460749 [00:47<00:01, 12467.30it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 439267/460749 [00:48<00:01, 12432.32it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 440507/460749 [00:48<00:01, 12126.81it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 441779/460749 [00:48<00:01, 11663.73it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 443155/460749 [00:48<00:01, 12076.55it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 444675/460749 [00:48<00:01, 12133.58it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 446051/460749 [00:48<00:01, 12416.13it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 447539/460749 [00:48<00:01, 12255.06it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 448931/460749 [00:48<00:00, 12565.53it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 450211/460749 [00:48<00:00, 11825.10it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 451699/460749 [00:49<00:00, 12521.03it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 452963/460749 [00:49<00:00, 11645.33it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 454579/460749 [00:49<00:00, 12837.13it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 455888/460749 [00:49<00:00, 12004.99it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 457507/460749 [00:49<00:00, 13065.91it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 458842/460749 [00:49<00:00, 12216.19it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 460371/460749 [00:49<00:00, 12900.04it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 460749/460749 [00:49<00:00, 9258.72it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/183 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/183 [00:00<00:14, 12.75it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 35/183 [00:00<00:01, 85.09it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 83/183 [00:00<00:00, 163.63it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 163/183 [00:00<00:00, 239.70it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 183/183 [00:00<00:00, 210.91it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 928.56it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 925.49it/s]\u001b[0m\n",
      "\u001b[34m<<<< load model from uie-base-en!!!\u001b[0m\n",
      "\u001b[34mW1013 09:35:34.628412    83 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2\u001b[0m\n",
      "\u001b[34mW1013 09:35:34.654865    83 gpu_resources.cc:91] device: 0, cuDNN Version: 8.0.\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:35:38,905] [    INFO]#033[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load '/opt/ml/checkpoints/'.#033[0m\u001b[0m\n",
      "\u001b[34m<<<< freeze the encoder layers!!!\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:35:53,415] [    INFO]#033[0m - global step 10, epoch: 2, loss: 0.00483, speed: 0.69 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:36:05,702] [    INFO]#033[0m - global step 20, epoch: 3, loss: 0.00361, speed: 0.81 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:36:18,037] [    INFO]#033[0m - global step 30, epoch: 4, loss: 0.00309, speed: 0.81 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:36:29,813] [    INFO]#033[0m - global step 40, epoch: 5, loss: 0.00264, speed: 0.85 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:36:42,567] [    INFO]#033[0m - global step 50, epoch: 7, loss: 0.00234, speed: 0.78 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:36:42,900] [    INFO]#033[0m - Evaluation precision: 0.00000, recall: 0.00000, F1: 0.00000#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:36:55,425] [    INFO]#033[0m - global step 60, epoch: 8, loss: 0.00216, speed: 0.80 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:37:08,014] [    INFO]#033[0m - global step 70, epoch: 9, loss: 0.00197, speed: 0.79 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:37:20,033] [    INFO]#033[0m - global step 80, epoch: 10, loss: 0.00183, speed: 0.83 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:37:33,052] [    INFO]#033[0m - global step 90, epoch: 12, loss: 0.00172, speed: 0.77 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:37:45,855] [    INFO]#033[0m - global step 100, epoch: 13, loss: 0.00161, speed: 0.78 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:37:46,188] [    INFO]#033[0m - Evaluation precision: 0.00000, recall: 0.00000, F1: 0.00000#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:37:59,053] [    INFO]#033[0m - global step 110, epoch: 14, loss: 0.00155, speed: 0.78 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:38:11,162] [    INFO]#033[0m - global step 120, epoch: 15, loss: 0.00148, speed: 0.83 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:38:24,090] [    INFO]#033[0m - global step 130, epoch: 17, loss: 0.00143, speed: 0.77 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:38:36,657] [    INFO]#033[0m - global step 140, epoch: 18, loss: 0.00137, speed: 0.80 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:38:49,140] [    INFO]#033[0m - global step 150, epoch: 19, loss: 0.00133, speed: 0.80 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:38:49,474] [    INFO]#033[0m - Evaluation precision: 0.25000, recall: 0.10000, F1: 0.14286#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:38:49,475] [    INFO]#033[0m - best F1 performence has been updated: 0.00000 --> 0.14286#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:38:50,256] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:38:50,256] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:39:02,088] [    INFO]#033[0m - global step 160, epoch: 20, loss: 0.00129, speed: 0.85 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:39:14,878] [    INFO]#033[0m - global step 170, epoch: 22, loss: 0.00126, speed: 0.78 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:39:27,399] [    INFO]#033[0m - global step 180, epoch: 23, loss: 0.00122, speed: 0.80 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:39:39,890] [    INFO]#033[0m - global step 190, epoch: 24, loss: 0.00119, speed: 0.80 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:39:51,743] [    INFO]#033[0m - global step 200, epoch: 25, loss: 0.00116, speed: 0.84 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:39:52,071] [    INFO]#033[0m - Evaluation precision: 0.00000, recall: 0.00000, F1: 0.00000#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:40:04,879] [    INFO]#033[0m - global step 210, epoch: 27, loss: 0.00114, speed: 0.78 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:40:17,428] [    INFO]#033[0m - global step 220, epoch: 28, loss: 0.00112, speed: 0.80 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:40:29,974] [    INFO]#033[0m - global step 230, epoch: 29, loss: 0.00110, speed: 0.80 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:40:41,880] [    INFO]#033[0m - global step 240, epoch: 30, loss: 0.00108, speed: 0.84 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:40:54,818] [    INFO]#033[0m - global step 250, epoch: 32, loss: 0.00106, speed: 0.77 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:40:55,146] [    INFO]#033[0m - Evaluation precision: 0.00000, recall: 0.00000, F1: 0.00000#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:41:07,827] [    INFO]#033[0m - global step 260, epoch: 33, loss: 0.00104, speed: 0.79 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:41:20,517] [    INFO]#033[0m - global step 270, epoch: 34, loss: 0.00103, speed: 0.79 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:41:32,600] [    INFO]#033[0m - global step 280, epoch: 35, loss: 0.00101, speed: 0.83 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:41:45,677] [    INFO]#033[0m - global step 290, epoch: 37, loss: 0.00099, speed: 0.76 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:41:58,524] [    INFO]#033[0m - global step 300, epoch: 38, loss: 0.00098, speed: 0.78 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:41:58,865] [    INFO]#033[0m - Evaluation precision: 0.00000, recall: 0.00000, F1: 0.00000#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:42:11,793] [    INFO]#033[0m - global step 310, epoch: 39, loss: 0.00097, speed: 0.77 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:42:24,177] [    INFO]#033[0m - global step 320, epoch: 40, loss: 0.00096, speed: 0.81 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:42:37,629] [    INFO]#033[0m - global step 330, epoch: 42, loss: 0.00095, speed: 0.74 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:42:50,919] [    INFO]#033[0m - global step 340, epoch: 43, loss: 0.00094, speed: 0.75 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:43:04,371] [    INFO]#033[0m - global step 350, epoch: 44, loss: 0.00093, speed: 0.74 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:43:04,715] [    INFO]#033[0m - Evaluation precision: 0.00000, recall: 0.00000, F1: 0.00000#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:43:17,711] [    INFO]#033[0m - global step 360, epoch: 45, loss: 0.00092, speed: 0.77 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:43:31,902] [    INFO]#033[0m - global step 370, epoch: 47, loss: 0.00091, speed: 0.70 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:43:45,802] [    INFO]#033[0m - global step 380, epoch: 48, loss: 0.00090, speed: 0.72 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:43:59,883] [    INFO]#033[0m - global step 390, epoch: 49, loss: 0.00089, speed: 0.71 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:44:13,345] [    INFO]#033[0m - global step 400, epoch: 50, loss: 0.00088, speed: 0.74 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-13 09:44:13,698] [    INFO]#033[0m - Evaluation precision: 0.00000, recall: 0.00000, F1: 0.00000#033[0m\u001b[0m\n",
      "\u001b[34m2022-10-13 09:44:20,886 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-10-13 09:44:20,886 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-10-13 09:44:20,887 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-10-13 09:45:06 Uploading - Uploading generated training model"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "hyperparameters = {'train_path': '/opt/ml/input/data/training/train.txt', \n",
    "                   'dev_path': '/opt/ml/input/data/training/dev.txt', \n",
    "                   'save_dir': '/opt/ml/model', \n",
    "                   'learning_rate': 1e-5, \n",
    "                   'batch_size': 16, \n",
    "                   'max_seq_len':512, \n",
    "                   'num_epochs': 50, \n",
    "                   'model': 'uie-base',\n",
    "                   'seed': 1000,\n",
    "                   'logging_steps': 10,\n",
    "                   'valid_steps': 50, # note this step should not larger than total\n",
    "                   'device': 'gpu',\n",
    "                   'freeze':True}\n",
    "\n",
    "instance_type = 'ml.g4dn.2xlarge'  # 'ml.p3.2xlarge' or 'ml.p3.8xlarge' or ...\n",
    "\n",
    "#git_config = {'repo': 'https://github.com/whn09/paddlenlp_sagemaker.git', 'branch': 'main'}\n",
    "\n",
    "estimator = PyTorch(entry_point='finetune.py',\n",
    "                    source_dir='./',\n",
    "                           # git_config=git_config,\n",
    "                    role=role,\n",
    "                    hyperparameters=hyperparameters,\n",
    "                    framework_version='1.9.1',\n",
    "                    py_version='py38',\n",
    "                    script_mode=True,\n",
    "                    instance_count=1,  # 1 or 2 or ...\n",
    "                    instance_type=instance_type,\n",
    "                    # Parameters required to enable checkpointing\n",
    "                    checkpoint_s3_uri=uie_en_model_s3, #使用你自己用来保存/加载模型的s3桶地址, 注意桶需要在us-east-1\n",
    "                    checkpoint_local_path=\"/opt/ml/checkpoints\")\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-training-2022-10-13-09-26-06-200\n"
     ]
    }
   ],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "# training_job_name = 'xxx'\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-064542430558/pytorch-training-2022-10-13-09-26-06-200/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data = estimator.model_data\n",
    "print (model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to prepare for predictions\n",
    "\n",
    "The deploy() method creates an endpoint (in this case locally) which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "usage: aws s3 cp <LocalPath> <S3Uri> or <S3Uri> <LocalPath> or <S3Uri> <S3Uri>\n",
      "Error: Invalid argument type\n",
      "model_150/\n",
      "model_250/\n",
      "inference.pdmodel\n",
      "model_350/\n",
      "model_200/\n",
      "model_400/\n",
      "inference.pdiparams.info\n",
      "inference.pdiparams\n",
      "model_100/\n",
      "model_50/\n",
      "model_300/\n",
      "model_best/\n",
      "model_best/vocab.txt\n",
      "model_best/special_tokens_map.json\n",
      "model_best/model_state.pdparams\n",
      "model_best/tokenizer_config.json\n",
      "model_best/model_config.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ${model_data} /tmp/\n",
    "!tar -zxvf /tmp/model.tar.gz -C /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/infer.py\n",
      "code/infer_gpu_shulex.py\n",
      "code/.ipynb_checkpoints/\n",
      "code/.ipynb_checkpoints/infer_gpu-checkpoint.py\n",
      "code/.ipynb_checkpoints/infer_gpu_shulex-checkpoint.py\n",
      "code/uie_predictor.py\n",
      "code/infer_cpu.py\n",
      "code/requirements.txt\n",
      "code/requirements_gpu.txt\n",
      "code/model.py\n",
      "code/infer_gpu.py\n",
      "code/requirements_cpu.txt\n",
      "inference.pdiparams\n",
      "inference.pdiparams.info\n",
      "inference.pdmodel\n",
      "model_config.json\n",
      "model_state.pdparams\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!cp /tmp/inference.* model/\n",
    "!cp /tmp/model_best/* model/\n",
    "!cp model/code/requirements_gpu.txt model/code/requirements.txt\n",
    "!cd model && tar -czvf ../model-inference-gpu.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model-inference-gpu.tar.gz to s3://sagemaker-us-west-2-064542430558/output/model-inference-gpu.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp model-inference-gpu.tar.gz s3://$bucket/output/model-inference-gpu.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "instance_type = 'ml.g4dn.xlarge'\n",
    "\n",
    "# predictor = estimator.deploy(initial_instance_count=1, instance_type=instance_type)\n",
    "\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data='s3://{}/output/model-inference-gpu.tar.gz'.format(bucket), role=role,\n",
    "                             entry_point='infer_gpu_shulex.py', framework_version='1.9.0', py_version='py38', model_server_workers=4)  # TODO [For GPU], model_server_workers=6\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type=instance_type, initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  [{'Shoe Type': [{'text': 'Retro knee-high boot', 'start': 210, 'end': 230, 'probability': 0.9831036329269409}], 'Shoe Heel Height': [{'text': 'approximately 13\"', 'start': 182, 'end': 199, 'probability': 0.44407209753990173}], 'Shoe Heel Type': [{'text': 'block heel', 'start': 256, 'end': 266, 'probability': 0.45025530457496643}], 'Shoe Toe Style': [{'text': 'square toe', 'start': 241, 'end': 251, 'probability': 0.9709370732307434}]}]\n",
      "time: 0.22002243995666504\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Funtasma by Pleaser Women's Gogo-300 Boot\\n100% Synthetic  \\n Manmade sole  \\n Shaft measures approximately 16 1/2\\\" from arch  \\n Heel measures approximately 3\\\"  \\n Boot opening measures approximately 13\\\" around  \\n Retro knee-high boot featuring square toe and block heel\"]\n",
    "import time\n",
    "start = time.time()\n",
    "outputs = predictor.predict(texts)\n",
    "end = time.time()\n",
    "print('outputs: ', outputs)\n",
    "print('time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [{\"id\": 40097, \"start_offset\": 28, \"end_offset\": 41, \"label\": \"Shoe Type\"}, {\"id\": 40098, \"start_offset\": 60, \"end_offset\": 74, \"label\": \"Shoe Pattern\"}, {\"id\": 40099, \"start_offset\": 126, \"end_offset\": 158, \"label\": \"Shoe Heel Height\"}, {\"id\": 40100, \"start_offset\": 210, \"end_offset\": 230, \"label\": \"Shoe Type\"}, {\"id\": 40101, \"start_offset\": 231, \"end_offset\": 251, \"label\": \"Shoe Toe Style\"}, {\"id\": 40102, \"start_offset\": 256, \"end_offset\": 266, \"label\": \"Shoe Heel Type\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = ['Shoe Type',\n",
    "     'Shoe Heel Height',\n",
    "     'Shoe Pattern',\n",
    "     'Shoe Heel Type',\n",
    "     'Shoe Toe Style']\n",
    "\n",
    "true = []\n",
    "for i in ls:\n",
    "    for j in label:\n",
    "        if j['label']==i:\n",
    "            true.append({'type':i,'text':texts[0][int(j['start_offset']):int(j['end_offset'])],'start':j['start_offset'],'end':j['end_offset']})\n",
    "            #true.append({'start':j['start_offset'],'end':j['end_offset']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Shoe Type', 'text': 'Gogo-300 Boot', 'start': 28, 'end': 41},\n",
       " {'type': 'Shoe Type',\n",
       "  'text': 'Retro knee-high boot',\n",
       "  'start': 210,\n",
       "  'end': 230},\n",
       " {'type': 'Shoe Heel Height',\n",
       "  'text': 'Heel measures approximately 3\"  ',\n",
       "  'start': 126,\n",
       "  'end': 158},\n",
       " {'type': 'Shoe Pattern', 'text': 'Manmade sole  ', 'start': 60, 'end': 74},\n",
       " {'type': 'Shoe Heel Type', 'text': 'block heel', 'start': 256, 'end': 266},\n",
       " {'type': 'Shoe Toe Style',\n",
       "  'text': 'featuring square toe',\n",
       "  'start': 231,\n",
       "  'end': 251}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare \n",
    "\n",
    "predict = []\n",
    "for i in ls:\n",
    "    try:\n",
    "        predict.append(outputs[0][i]) \n",
    "    except:\n",
    "        predict.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'Retro knee-high boot',\n",
       "   'start': 210,\n",
       "   'end': 230,\n",
       "   'probability': 0.9831036329269409}],\n",
       " [{'text': 'approximately 13\"',\n",
       "   'start': 182,\n",
       "   'end': 199,\n",
       "   'probability': 0.44407209753990173}],\n",
       " '',\n",
       " [{'text': 'block heel',\n",
       "   'start': 256,\n",
       "   'end': 266,\n",
       "   'probability': 0.45025530457496643}],\n",
       " [{'text': 'square toe',\n",
       "   'start': 241,\n",
       "   'end': 251,\n",
       "   'probability': 0.9709370732307434}]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "Deleting the local endpoint when you're finished is important since you can only run one local endpoint at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.delete_endpoint()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"I wipe whatever tears had trickled down my face, removing my rings from my fingers and clutching them in my hands.\\nThe hallway seems longer than normal but I walk briskly to the office where I find Christian, the elders, the lawyer, Jordan, Derek and Vanessa waiting for me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
